@article{yannakakis,
author = {Papadimitriou, Christos H. and Yannakakis, Mihalis},
title = {On the Complexity of Database Queries},
year = {1999},
issue_date = {June 1999},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {58},
number = {3},
issn = {0022-0000},
url = {https://doi.org/10.1006/jcss.1999.1626},
doi = {10.1006/jcss.1999.1626},
abstract = {We revisit the issue of the complexity of database queries, in the light of the recent parametric refinement of complexity theory. We show that, if the query size (or the number of variables in the query) is considered as a parameter, then the relational calculus and its fragments (conjunctive queries, positive queries) are classified at appropriate levels of the so-called W hierarchy of Downey and Fellows. These results strongly suggest that the query size is inherently in the exponent of the data complexity of any query evaluation algorithm, with the implication becoming stronger as the expressibility of the query language increases. On the positive side, we show that this exponential dependence can be avoided for the extension of acyclic queries with (but not &lt;) inequalities.},
journal = {J. Comput. Syst. Sci.},
month = jun,
pages = {407–427},
numpages = {21}
}

@inproceedings{2020-pldi-szalinski-cad-eqsat,
  author    = {Chandrakana Nandi and
               Max Willsey and
               Adam Anderson and
               James R. Wilcox and
               Eva Darulova and
               Dan Grossman and
               Zachary Tatlock},
  editor    = {Alastair F. Donaldson and
               Emina Torlak},
  title     = {Synthesizing Structured {CAD} Models with Equality Saturation and
               Inverse Transformations},
  booktitle = {Proceedings of the 41st {ACM} {SIGPLAN} International Conference on
               Programming Language Design and Implementation, {PLDI} 2020,
               London, UK, June 15-20, 2020},
  pages     = {31--44},
  publisher = {{ACM}},
  year      = {2020},
  url       = {https://doi.org/10.1145/3385412.3386012},
  doi       = {10.1145/3385412.3386012},
}

@ARTICLE{tensat,
       author = {{Yang}, Yichen and {Mangpo Phothilimtha}, Phitchaya and {Remy Wang}, Yisu and {Willsey}, Max and {Roy}, Sudip and {Pienaar}, Jacques},
        title = "{Equality Saturation for Tensor Graph Superoptimization}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing},
         year = 2021,
        month = jan,
          eid = {arXiv:2101.01332},
        pages = {arXiv:2101.01332},
archivePrefix = {arXiv},
       eprint = {2101.01332},
 primaryClass = {cs.AI},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv210101332Y},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{datalog-survey,
author = {Green, Todd J. and Huang, Shan Shan and Loo, Boon Thau and Zhou, Wenchao},
title = {Datalog and Recursive Query Processing},
year = {2013},
issue_date = {November 2013},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {5},
number = {2},
issn = {1931-7883},
url = {https://doi.org/10.1561/1900000017},
doi = {10.1561/1900000017},
abstract = {In recent years, we have witnessed a revival of the use of recursive queries in a variety of emerging application domains such as data integration and exchange, information extraction, networking, and program analysis. A popular language used for expressing these queries is Datalog. This paper surveys for a general audience the Datalog language, recursive query processing, and optimization techniques. This survey differs from prior surveys written in the eighties and nineties in its comprehensiveness of topics, its coverage of recent developments and applications, and its emphasis on features and techniques beyond "classical" Datalog which are vital for practical applications. Specifically, the topics covered include the core Datalog language and various extensions, semantics, query optimizations, magic-sets optimizations, incremental view maintenance, aggregates, negation, and types. We conclude the paper with a survey of recent systems and applications that use Datalog and recursive queries.},
journal = {Found. Trends Databases},
month = nov,
pages = {105–195},
numpages = {91}
}

@article{emptyheaded,
author = {Aberger, Christopher R. and Lamb, Andrew and Tu, Susan and N\"{o}tzli, Andres and Olukotun, Kunle and R\'{e}, Christopher},
title = {EmptyHeaded: A Relational Engine for Graph Processing},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0362-5915},
url = {https://doi.org/10.1145/3129246},
doi = {10.1145/3129246},
abstract = {There are two types of high-performance graph processing engines: low- and high-level engines. Low-level engines (Galois, PowerGraph, Snap) provide optimized data structures and computation models but require users to write low-level imperative code, hence ensuring that efficiency is the burden of the user. In high-level engines, users write in query languages like datalog (SociaLite) or SQL (Grail). High-level engines are easier to use but are orders of magnitude slower than the low-level graph engines. We present EmptyHeaded, a high-level engine that supports a rich datalog-like query language and achieves performance comparable to that of low-level engines. At the core of EmptyHeaded’s design is a new class of join algorithms that satisfy strong theoretical guarantees, but have thus far not achieved performance comparable to that of specialized graph processing engines. To achieve high performance, EmptyHeaded introduces a new join engine architecture, including a novel query optimizer and execution engine that leverage single-instruction multiple data (SIMD) parallelism. With this architecture, EmptyHeaded outperforms high-level approaches by up to three orders of magnitude on graph pattern queries, PageRank, and Single-Source Shortest Paths (SSSP) and is an order of magnitude faster than many low-level baselines. We validate that EmptyHeaded competes with the best-of-breed low-level engine (Galois), achieving comparable performance on PageRank and at most 3\texttimes{} worse performance on SSSP. Finally, we show that the EmptyHeaded design can easily be extended to accommodate a standard resource description framework (RDF) workload, the LUBM benchmark. On the LUBM benchmark, we show that EmptyHeaded can compete with and sometimes outperform two high-level, but specialized RDF baselines (TripleBit and RDF-3X), while outperforming MonetDB by up to three orders of magnitude and LogicBlox by up to two orders of magnitude.},
journal = {ACM Trans. Database Syst.},
month = oct,
articleno = {20},
numpages = {44},
keywords = {generalized hypertree decomposition, Worst-case optimal join, graph processing, single-instruction multiple data, GHD, SIMD}
}


@mastersthesis{eval-wcoj,
author = {Andreas Amler},
title = {Evaluation of Worst-Case Optimal Join
Algorithm},
publisher = {Department of Informatics,
Technische Universit\"at M\"unchen},
year = {2017}
}

@article{egg,
author = {Willsey, Max and Nandi, Chandrakana and Wang, Yisu Remy and Flatt, Oliver and Tatlock, Zachary and Panchekha, Pavel},
title = {Egg: Fast and Extensible Equality Saturation},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {POPL},
url = {https://doi.org/10.1145/3434304},
doi = {10.1145/3434304},
abstract = {An e-graph efficiently represents a congruence relation over many expressions. Although they were originally developed in the late 1970s for use in automated theorem provers, a more recent technique known as equality saturation repurposes e-graphs to implement state-of-the-art, rewrite-driven compiler optimizations and program synthesizers. However, e-graphs remain unspecialized for this newer use case. Equality saturation workloads exhibit distinct characteristics and often require ad-hoc e-graph extensions to incorporate transformations beyond purely syntactic rewrites.  This work contributes two techniques that make e-graphs fast and extensible, specializing them to equality saturation. A new amortized invariant restoration technique called rebuilding takes advantage of equality saturation's distinct workload, providing asymptotic speedups over current techniques in practice. A general mechanism called e-class analyses integrates domain-specific analyses into the e-graph, reducing the need for ad hoc manipulation.  We implemented these techniques in a new open-source library called egg. Our case studies on three previously published applications of equality saturation highlight how egg's performance and flexibility enable state-of-the-art results across diverse domains.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {23},
numpages = {29},
keywords = {e-graphs, equality saturation}
}

@inproceedings{fpa,
author = {Kozen, Dexter},
title = {Complexity of Finitely Presented Algebras},
year = {1977},
isbn = {9781450374095},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.offcampus.lib.washington.edu/10.1145/800105.803406},
doi = {10.1145/800105.803406},
abstract = {An algebra A is finitely presented if there is a finite set G of generator symbols, a finite set O of operator symbols, and a finite set Γ of defining relations xΞy where x and y are well-formed terms over G and O, such that A is isomorphic to the free algebra on G and O modulo the congruence induced by Γ.The uniform word problem, the finiteness problem, the triviality problem (whether A is the one element algebra), and the subalgebra membership problem (whether a given element of A is contained in a finitely generated subalgebra of A) for finitely presented algebras are shown to be ≤mlog-complete for P. The schema satisfiability problem and schema validity problem are shown to be ≤mlog-complete for NP and co-NP, respectively. Finally, the problem of isomorphism of finitely presented algebras is shown to be polynomial time many-one equivalent to the problem of graph isomorphism.},
booktitle = {Proceedings of the Ninth Annual ACM Symposium on Theory of Computing},
pages = {164–177},
numpages = {14},
location = {Boulder, Colorado, USA},
series = {STOC '77}
}

@inproceedings{agm,
author = {Atserias, Albert and Grohe, Martin and Marx, D\'{a}niel},
title = {Size Bounds and Query Plans for Relational Joins},
year = {2008},
isbn = {9780769534367},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/FOCS.2008.43},
doi = {10.1109/FOCS.2008.43},
abstract = {Relational joins are at the core of relational algebra, which in turn is the core of the standard database query language SQL. As their evaluation is expensive and very often dominated by the output size, it is an important task for database query optimisers to compute estimates on the size of joins and to find good execution plans for sequences of joins. We study these problems from a theoretical perspective, both in the worst-case model, and in an average-case model where the database is chosen according to a known probability distribution. In the former case, our first key observation is that the worst-case size of a query is characterised by the fractional edge cover number of its underlying hypergraph, a combinatorial parameter previously known to provide an upper bound. We complete the picture by proving a matching lower bound, and by showing that there exist queries for which the join-project plan suggested by the fractional edge cover approach may be substantially better than any join plan that does not use intermediate projections.},
booktitle = {Proceedings of the 2008 49th Annual IEEE Symposium on Foundations of Computer Science},
pages = {739–748},
numpages = {10},
series = {FOCS '08}
}

@article{wcoj,
author = {Ngo, Hung Q. and Porat, Ely and R\'{e}, Christopher and Rudra, Atri},
title = {Worst-Case Optimal Join Algorithms},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {3},
issn = {0004-5411},
url = {https://doi.org/10.1145/3180143},
doi = {10.1145/3180143},
abstract = {Efficient join processing is one of the most fundamental and well-studied tasks in database research. In this work, we examine algorithms for natural join queries over many relations and describe a new algorithm to process these queries optimally in terms of worst-case data complexity. Our result builds on recent work by Atserias, Grohe, and Marx, who gave bounds on the size of a natural join query in terms of the sizes of the individual relations in the body of the query. These bounds, however, are not constructive: they rely on Shearer’s entropy inequality, which is information-theoretic. Thus, the previous results leave open the question of whether there exist algorithms whose runtimes achieve these optimal bounds. An answer to this question may be interesting to database practice, as we show in this article that any project-join style plans, such as ones typically employed in a relational database management system, are asymptotically slower than the optimal for some queries. We present an algorithm whose runtime is worst-case optimal for all natural join queries. Our result may be of independent interest, as our algorithm also yields a constructive proof of the general fractional cover bound by Atserias, Grohe, and Marx without using Shearer’s inequality. This bound implies two famous inequalities in geometry: the Loomis-Whitney inequality and its generalization, the Bollob\'{a}s-Thomason inequality. Hence, our results algorithmically prove these inequalities as well. Finally, we discuss how our algorithm can be used to evaluate full conjunctive queries optimally, to compute a relaxed notion of joins and to optimally (in the worst-case) enumerate all induced copies of a fixed subgraph inside of a given large graph.},
journal = {J. ACM},
month = mar,
articleno = {16},
numpages = {40},
keywords = {Loomis-Whitney inequality, fractional cover bound, Bollob\'{a}s-Thomason inequality, Join Algorithms}
}

@InProceedings{Z3,
author="de Moura, Leonardo
and Bj{\o}rner, Nikolaj",
editor="Ramakrishnan, C. R.
and Rehof, Jakob",
title="Z3: An Efficient SMT Solver",
booktitle="Tools and Algorithms for the Construction and Analysis of Systems",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="337--340",
abstract="Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications.",
isbn="978-3-540-78800-3"
}



@inproceedings{CVC4,
  author    = {Clark W. Barrett and
               Christopher L. Conway and
               Morgan Deters and
               Liana Hadarean and
               Dejan Jovanovic and
               Tim King and
               Andrew Reynolds and
               Cesare Tinelli},
  editor    = {Ganesh Gopalakrishnan and
               Shaz Qadeer},
  title     = {{CVC4}},
  booktitle = {Computer Aided Verification - 23rd International Conference, {CAV}
               2011, Snowbird, UT, USA, July 14-20, 2011. Proceedings},
  series    = {Lecture Notes in Computer Science},
  volume    = {6806},
  pages     = {171--177},
  publisher = {Springer},
  year      = {2011},
  url       = {https://doi.org/10.1007/978-3-642-22110-1\_14},
  doi       = {10.1007/978-3-642-22110-1\_14},
  biburl    = {https://dblp.org/rec/conf/cav/BarrettCDHJKRT11.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{proof-producing,
author = {Nieuwenhuis, Robert and Oliveras, Albert},
title = {Proof-Producing Congruence Closure},
year = {2005},
isbn = {3540255966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-32033-3_33},
doi = {10.1007/978-3-540-32033-3_33},
abstract = {Many applications of congruence closure nowadays require the ability of recovering, among the thousands of input equations, the small subset that caused the equivalence of a given pair of terms. For this purpose, here we introduce an incremental congruence closure algorithm that has an additional $mathit{Explain}$ operation.First, two variations of union-find data structures with $mathit{Explain}$ are introduced. Then, these are applied inside a congruence closure algorithm with $mathit{Explain}$, where a k-step proof can be recovered in almost optimal time (quasi-linear in k), without increasing the overall O(n log n) runtime of the fastest known congruence closure algorithms.This non-trivial (ground) equational reasoning result has been quite intensively sought after (see, e.g., [SD99,dMRS04,KS04]), and moreover has important applications to verification.},
booktitle = {Proceedings of the 16th International Conference on Term Rewriting and Applications},
pages = {453–468},
numpages = {16},
location = {Nara, Japan},
series = {RTA'05}
}

@article{simplify,
author = {Detlefs, David and Nelson, Greg and Saxe, James B.},
title = {Simplify: A Theorem Prover for Program Checking},
year = {2005},
issue_date = {May 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {3},
issn = {0004-5411},
url = {https://doi.org/10.1145/1066100.1066102},
doi = {10.1145/1066100.1066102},
abstract = {This article provides a detailed description of the automatic theorem prover Simplify, which is the proof engine of the Extended Static Checkers ESC/Java and ESC/Modula-3. Simplify uses the Nelson--Oppen method to combine decision procedures for several important theories, and also employs a matcher to reason about quantifiers. Instead of conventional matching in a term DAG, Simplify matches up to equivalence in an E-graph, which detects many relevant pattern instances that would be missed by the conventional approach. The article describes two techniques, error context reporting and error localization, for helping the user to determine the reason that a false conjecture is false. The article includes detailed performance figures on conjectures derived from realistic program-checking problems.},
journal = {J. ACM},
month = may,
pages = {365–473},
numpages = {109},
keywords = {decision procedures, program checking, Theorem proving}
}

@article{tarjan,
author = {Tarjan, Robert Endre},
title = {Efficiency of a Good But Not Linear Set Union Algorithm},
year = {1975},
issue_date = {April 1975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0004-5411},
url = {https://doi.org/10.1145/321879.321884},
doi = {10.1145/321879.321884},
journal = {J. ACM},
month = apr,
pages = {215–225},
numpages = {11}
}

@inproceedings{semsearch,
author = {Premtoon, Varot and Koppel, James and Solar-Lezama, Armando},
title = {Semantic Code Search via Equational Reasoning},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386001},
doi = {10.1145/3385412.3386001},
abstract = {We present a new approach to semantic code search based on equational reasoning, and the Yogo tool implementing this approach. Our approach works by considering not only the dataflow graph of a function, but also the dataflow graphs of all equivalent functions reachable via a set of rewrite rules. In doing so, it can recognize an operation even if it uses alternate APIs, is in a different but mathematically-equivalent form, is split apart with temporary variables, or is interleaved with other code. Furthermore, it can recognize when code is an instance of some higher-level concept such as iterating through a file. Because of this, from a single query, Yogo can find equivalent code in multiple languages. Our evaluation further shows the utility of Yogo beyond code search: encoding a buggy pattern as a Yogo query, we found a bug in Oracle’s Graal compiler which had been missed by a hand-written static analyzer designed for that exact kind of bug. Yogo is built on the Cubix multi-language infrastructure, and currently supports Java and Python.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1066–1082},
numpages = {17},
keywords = {equational reasoning, code search},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{herbie,
author = {Panchekha, Pavel and Sanchez-Stern, Alex and Wilcox, James R. and Tatlock, Zachary},
title = {Automatically Improving Accuracy for Floating Point Expressions},
year = {2015},
isbn = {9781450334686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737924.2737959},
doi = {10.1145/2737924.2737959},
abstract = { Scientific and engineering applications depend on floating point arithmetic to approximate real arithmetic. This approximation introduces rounding error, which can accumulate to produce unacceptable results. While the numerical methods literature provides techniques to mitigate rounding error, applying these techniques requires manually rearranging expressions and understanding the finer details of floating point arithmetic. We introduce Herbie, a tool which automatically discovers the rewrites experts perform to improve accuracy. Herbie's heuristic search estimates and localizes rounding error using sampled points (rather than static error analysis), applies a database of rules to generate improvements, takes series expansions, and combines improvements for different input regions. We evaluated Herbie on examples from a classic numerical methods textbook, and found that Herbie was able to improve accuracy on each example, some by up to 60 bits, while imposing a median performance overhead of 40%. Colleagues in machine learning have used Herbie to significantly improve the results of a clustering algorithm, and a mathematical library has accepted two patches generated using Herbie. },
booktitle = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1–11},
numpages = {11},
keywords = {program rewriting, numerical accuracy, Floating point},
location = {Portland, OR, USA},
series = {PLDI '15}
}



@article{spores,
author = {Wang, Yisu Remy and Hutchison, Shana and Leang, Jonathan and Howe, Bill and Suciu, Dan},
title = {SPORES: Sum-Product Optimization via Relational Equality Saturation for Large Scale Linear Algebra},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3407790.3407799},
doi = {10.14778/3407790.3407799},
abstract = {Machine learning algorithms are commonly specified in linear algebra (LA). LA expressions can be rewritten into more efficient forms, by taking advantage of input properties such as sparsity, as well as program properties such as common subexpressions and fusible operators. The complex interaction among these properties' impact on the execution cost poses a challenge to optimizing compilers. Existing compilers resort to intricate heuristics that complicate the codebase and add maintenance cost, but fail to search through the large space of equivalent LA expressions to find the cheapest one. We introduce a general optimization technique for LA expressions, by converting the LA expressions into Relational Algebra (RA) expressions, optimizing the latter, then converting the result back to (optimized) LA expressions. The rewrite rules we design in this approach are complete, meaning that any equivalent LA expression is covered in the search space. The challenge is the major size of the search space, and we address this by adopting and extending a technique used in compilers, called equality saturation. Our optimizer, SPORES, uses rule sampling to quickly cover vast portions of the search space; it then uses a constraint solver to extract the optimal plan from the covered space, or alternatively uses a greedy algorithm to shorten compile time. We integrate SPORES into SystemML and validate it empirically across a spectrum of machine learning tasks; SPORES can derive all existing hand-coded optimizations in SystemML, and perform new optimizations that lead to up to 10X speedup.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {1919–1932},
numpages = {14}
}

@article{denaili,
author = {Joshi, Rajeev and Nelson, Greg and Zhou, Yunhong},
title = {Denali: A Practical Algorithm for Generating Optimal Code},
year = {2006},
issue_date = {November 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {6},
issn = {0164-0925},
url = {https://doi.org/10.1145/1186632.1186633},
doi = {10.1145/1186632.1186633},
abstract = {This article presents a design for the Denali-2 superoptimizer, which will generate minimum-instruction-length machine code for realistic machine architectures using automatic theorem-proving technology: specifically, using E-graph matching (a technique for pattern matching in the presence of equality information) and Boolean satisfiability solving.This article presents a precise definition of the underlying automatic programming problem solved by the Denali-2 superoptimizer. It sketches the E-graph matching phase and presents a detailed exposition and proof of soundness of the reduction of the automatic programming problem to the Boolean satisfiability problem.},
journal = {ACM Trans. Program. Lang. Syst.},
month = nov,
pages = {967–989},
numpages = {23},
keywords = {practical optimal code generation, code generation, Compilation}
}

@inproceedings{cc-in-tt,
author = {Selsam, Daniel and Moura, Leonardo},
title = {Congruence Closure in Intensional Type Theory},
year = {2016},
isbn = {9783319402284},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-40229-1_8},
doi = {10.1007/978-3-319-40229-1_8},
abstract = {Congruence closure procedures are used extensively in automated reasoning and are a core component of most satisfiability modulo theories solvers. However, no known congruence closure algorithms can support any of the expressive logics based on intensional type theory ITT, which form the basis of many interactive theorem provers. The main source of expressiveness in these logics is dependent types, and yet existing congruence closure procedures found in interactive theorem provers based on ITT do not handle dependent types at all and only work on the simply-typed subsets of the logics. Here we present an efficient and proof-producing congruence closure procedure that applies to every function in ITT no matter how many dependencies exist among its arguments, and that only relies on the commonly assumed uniqueness of identity proofs axiom. We demonstrate its usefulness by solving interesting verification problems involving functions with dependent types.},
booktitle = {Proceedings of the 8th International Joint Conference on Automated Reasoning - Volume 9706},
pages = {99–115},
numpages = {17}
}

@InProceedings{efficient-ematching,
author="de Moura, Leonardo
and Bj{\o}rner, Nikolaj",
editor="Pfenning, Frank",
title="Efficient E-Matching for SMT Solvers",
booktitle="Automated Deduction -- CADE-21",
year="2007",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="183--198",
abstract="Satisfiability Modulo Theories (SMT) solvers have proven highly scalable, efficient and suitable for integrating theory reasoning. However, for numerous applications from program analysis and verification, the ground fragment is insufficient, as proof obligations often include quantifiers. A well known approach for quantifier reasoning uses a matching algorithm that works against an E-graph to instantiate quantified variables. This paper introduces algorithms that identify matches on E-graphs incrementally and efficiently. In particular, we introduce an index that works on E-graphs, called E-matching code trees that combine features of substitution and code trees, used in saturation based theorem provers. E-matching code trees allow performing matching against several patterns simultaneously. The code trees are combined with an additional index, called the inverted path index, which filters E-graph terms that may potentially match patterns when the E-graph is updated. Experimental results show substantial performance improvements over existing state-of-the-art SMT solvers.",
isbn="978-3-540-73595-3"
}

@phdthesis{eqsatthesis,
author = {Nelson, Charles Gregory},
title = {Techniques for Program Verification},
year = {1980},
publisher = {Stanford University},
address = {Stanford, CA, USA},
note = {AAI8011683}
}

@inproceedings{eqsat,
author = {Tate, Ross and Stepp, Michael and Tatlock, Zachary and Lerner, Sorin},
title = {Equality Saturation: A New Approach to Optimization},
year = {2009},
isbn = {9781605583792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1480881.1480915},
doi = {10.1145/1480881.1480915},
abstract = {Optimizations in a traditional compiler are applied sequentially, with each optimization destructively modifying the program to produce a transformed program that is then passed to the next optimization. We present a new approach for structuring the optimization phase of a compiler. In our approach, optimizations take the form of equality analyses that add equality information to a common intermediate representation. The optimizer works by repeatedly applying these analyses to infer equivalences between program fragments, thus saturating the intermediate representation with equalities. Once saturated, the intermediate representation encodes multiple optimized versions of the input program. At this point, a profitability heuristic picks the final optimized program from the various programs represented in the saturated representation. Our proposed way of structuring optimizers has a variety of benefits over previous approaches: our approach obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than our own. We present our approach, formalize it, and describe our choice of intermediate representation. We also present experimental results showing that our approach is practical in terms of time and space overhead, is effective at discovering intricate optimization opportunities, and is effective at performing translation validation for a realistic optimizer.},
booktitle = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {264–276},
numpages = {13},
keywords = {equality reasoning, compiler optimization, intermediate representation},
location = {Savannah, GA, USA},
series = {POPL '09}
}